% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}


\title{Installation and Configuration Documentation}
\date{June 30, 2014}
\release{1.0.1}
\author{Oshin Prem}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


Contents:


\chapter{HADOOP INSTALLATION}
\label{hadoop:welcome-to-installation-and-configuration-s-documentation}\label{hadoop::doc}\label{hadoop:hadoop-installation}
This section refers to the installation settings of Hadoop on a standalone system
as well as on a system existing as a node in a cluster.


\section{SINGLE-NODE INSTALLATION}
\label{hadoop:single-node-installation}

\subsection{Running Hadoop on Ubuntu (Single node cluster setup)}
\label{hadoop:running-hadoop-on-ubuntu-single-node-cluster-setup}
The report here will describe the required steps for setting up a single-node Hadoop cluster backed by the Hadoop Distributed File System, running on Ubuntu Linux.
Hadoop is a framework written in Java for running applications on large clusters of commodity hardware and incorporates features similar to those of the Google File System (GFS) and of the MapReduce computing paradigm. Hadoop’s HDFS is a highly fault-tolerant distributed file system and, like Hadoop in general, designed to be deployed on low-cost hardware. It provides high throughput access to application data and is suitable for applications that have large data sets.

Before we start, we will understand the meaning of the following:


\subsubsection{DataNode:}
\label{hadoop:datanode}
A DataNode stores data in the Hadoop File System. A functional file system has more than one DataNode, with the data replicated across them.


\subsubsection{NameNode:}
\label{hadoop:namenode}
The NameNode is the centrepiece of an HDFS file system. It keeps the directory of all files in the file system, and tracks where across the cluster the file data is kept. It does not store the data of these file itself.


\subsubsection{Jobtracker:}
\label{hadoop:jobtracker}
The Jobtracker is the service within hadoop that farms out MapReduce to specific nodes in the cluster, ideally the nodes that have the data, or atleast are in the same rack.


\subsubsection{TaskTracker:}
\label{hadoop:tasktracker}
A TaskTracker is a node in the cluster that accepts tasks- Map, Reduce and Shuffle operatons – from a Job Tracker.


\subsubsection{Secondary Namenode:}
\label{hadoop:secondary-namenode}
Secondary Namenode whole purpose is to have a checkpoint in HDFS. It is just a helper node for namenode.


\subsection{Prerequisites}
\label{hadoop:prerequisites}

\subsubsection{Java 6 JDK}
\label{hadoop:java-6-jdk}
Hadoop requires a working Java 1.5+ (aka Java 5) installation.

Update the source list

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  sudo apt\PYGZhy{}get update
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{1.png}}
\end{figure}

or

Install Sun Java 6 JDK
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{2.png}}
\end{figure}


\paragraph{Note:}
\label{hadoop:note}
If you already have Java JDK installed on your system, then you need not run the above command.

To install it

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo apt\PYGZhy{}get install sun\PYGZhy{}java6\PYGZhy{}jdk
\end{Verbatim}

The full JDK which will be placed in /usr/lib/jvm/java-6-openjdk-amd64
After installation, check whether java JDK is correctly installed or not, with the following command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} java \PYGZhy{}version
\end{Verbatim}


\subsubsection{Adding a dedicated Hadoop system user}
\label{hadoop:adding-a-dedicated-hadoop-system-user}
We will use a dedicated Hadoop user account for running Hadoop.

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo addgroup hadoop\PYGZus{}group
user@ubuntu:\PYGZti{}\PYGZdl{} sudo adduser \PYGZhy{}\PYGZhy{}ingroup hadoop\PYGZus{}group hduser1
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{3.png}}
\end{figure}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{4.png}}
\end{figure}

This will add the user hduser1 and the group hadoop\_group to the local machine.
Add hduser1 to the sudo group

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo adduser hduser1 sudo
\end{Verbatim}


\subsubsection{Configuring SSH}
\label{hadoop:configuring-ssh}
The hadoop control scripts rely on SSH to peform cluster-wide operations. For example, there is a script for stopping and starting all the daemons in the clusters. To work seamlessly, SSh needs to be etup to allow password-less login for the hadoop user from machines in the cluster. The simplest ay to achive this is to generate a public/private key pair, and it will be shared across the cluster.

Hadoop requires SSH access to manage its nodes, i.e. remote machines plus your local machine. For our single-node setup of Hadoop, we therefore need to configure SSH access to localhost for the hduser user we created in the earlier.

We have to generate an SSH key for the hduser user.

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} su – hduser1
hduser1@ubuntu:\PYGZti{}\PYGZdl{} ssh\PYGZhy{}keygen \PYGZhy{}t rsa \PYGZhy{}P \PYGZdq{}\PYGZdq{}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{5.png}}
\end{figure}

The second line will create an RSA key pair with an empty password.


\paragraph{Note:}
\label{hadoop:id1}
P “”, here indicates an empty password

You have to enable SSH access to your local machine with this newly created key which is done by the following command.

\begin{Verbatim}[commandchars=\\\{\}]
hduser1@ubuntu:\PYGZti{}\PYGZdl{}   cat \PYGZdl{}HOME/.ssh/id\PYGZus{}rsa.pub \PYGZgt{}\PYGZgt{} \PYGZdl{}HOME/.ssh/authorized\PYGZus{}keys
\end{Verbatim}

The final step is to test the SSH setup by connecting to the local machine with the hduser1 user.
The step is also needed to save your local machine’s host key fingerprint to the hduser user’s known hosts file.

\begin{Verbatim}[commandchars=\\\{\}]
hduser@ubuntu:\PYGZti{}\PYGZdl{} ssh localhost
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{6.png}}
\end{figure}

If the SSH connection fails, we can try the following (optional):
\begin{itemize}
\item {} 
Enable debugging with ssh -vvv localhost and investigate the error in detail.

\item {} 
Check the SSH server configuration in /etc/ssh/sshd\_config.  If you made any changes to the SSH server configuration file, you can force a configuration reload with sudo /etc/init.d/ssh reload.

\end{itemize}


\subsection{INSTALLATION}
\label{hadoop:installation}

\subsubsection{Main Installation}
\label{hadoop:main-installation}\begin{itemize}
\item {} 
Now, I will start by switching to hduser

\begin{Verbatim}[commandchars=\\\{\}]
hduser@ubuntu:\PYGZti{}\PYGZdl{} su \PYGZhy{} hduser1
\end{Verbatim}

\item {} 
Now, download and extract Hadoop 1.2.0

\item {} 
Setup Environment Variables for Hadoop

\end{itemize}

Add the following entries to .bashrc file

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} Set Hadoop\PYGZhy{}related environment variables
export HADOOP\PYGZus{}HOME=/usr/local/hadoop
\PYGZsh{} Add Hadoop bin/ directory to PATH
export PATH= \PYGZdl{}PATH:\PYGZdl{}HADOOP\PYGZus{}HOME/bin
\end{Verbatim}


\subsubsection{Configuration}
\label{hadoop:configuration}

\paragraph{hadoop-env.sh}
\label{hadoop:hadoop-env-sh}
Change the file:
conf/hadoop-env.sh

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c}{\PYGZsh{}export JAVA\PYGZus{}HOME=/usr/lib/j2sdk1.5\PYGZhy{}sun}
\end{Verbatim}

to
in the same file

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c}{\PYGZsh{} export JAVA\PYGZus{}HOME=/usr/lib/jvm/java\PYGZhy{}6\PYGZhy{}openjdk\PYGZhy{}amd64  (for 64 bit)}
\PYG{c}{\PYGZsh{} export JAVA\PYGZus{}HOME=/usr/lib/jvm/java\PYGZhy{}6\PYGZhy{}openjdk\PYGZhy{}amd64  (for 32 bit)}
\end{Verbatim}


\paragraph{conf/{\color{red}\bfseries{}*}-site.xml}
\label{hadoop:conf-site-xml}
Now we create the directory and set the required ownerships and permissions

\begin{Verbatim}[commandchars=\\\{\}]
hduser@ubuntu:\PYGZti{}\PYGZdl{} sudo mkdir \PYGZhy{}p /app/hadoop/tmp
hduser@ubuntu:\PYGZti{}\PYGZdl{} sudo chown hduser:hadoop /app/hadoop/tmp
hduser@ubuntu:\PYGZti{}\PYGZdl{} sudo chmod 750 /app/hadoop/tmp
\end{Verbatim}

The last line gives reading and writing permissions to the /app/hadoop/tmp directory
\begin{itemize}
\item {} 
Error: If you forget to set the required ownerships and permissions, you will see a java.io.IO Exception when you try to format the name node.

\end{itemize}

Paste the following between \textless{}configuration\textgreater{}
\begin{itemize}
\item {} 
In file conf/core-site.xml

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}hadoop.tmp.dir\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}/app/hadoop/tmp\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}A base for other temporary directories.\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}

\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}fs.default.name\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}hdfs://localhost:54310\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}The name of the default file system.  A URI whose
    scheme and authority determine the FileSystem implementation.  The
    uri\PYGZsq{}s scheme determines the config property (fs.SCHEME.impl) naming
    the FileSystem implementation class.  The uri\PYGZsq{}s authority is used to
    determine the host, port, etc. for a filesystem.\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}

\item {} 
In file conf/mapred-site.xml

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
\PYGZlt{}name\PYGZgt{}mapred.job.tracker\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}localhost:54311\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}The host and port that the MapReduce job tracker runs
    at.  If \PYGZdq{}local\PYGZdq{}, then jobs are run in\PYGZhy{}process as a single map
    and reduce task.
    \PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}

\item {} 
In file conf/hdfs-site.xml

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}dfs.replication\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}1\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}Default block replication.
    The actual number of replications can be specified when the file is created.
    The default is used if replication is not specified in create time.
    \PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}

\end{itemize}


\subsubsection{Formatting the HDFS filesystem via the NameNode}
\label{hadoop:formatting-the-hdfs-filesystem-via-the-namenode}
To format the filesystem (which simply initializes the directory specified by the dfs.name.dir variable).
Run the command

\begin{Verbatim}[commandchars=\\\{\}]
hduser@ubuntu:\PYGZti{}\PYGZdl{} /usr/local/hadoop/bin/hadoop namenode –format
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{7.png}}
\end{figure}


\subsubsection{Starting your single-node cluster}
\label{hadoop:starting-your-single-node-cluster}
Before starting the cluster, we need to give the required permissions to the directory with the following command

\begin{Verbatim}[commandchars=\\\{\}]
hduser@ubuntu:\PYGZti{}\PYGZdl{} sudo chmod \PYGZhy{}R 777 /usr/local/hadoop
\end{Verbatim}

Run the command

\begin{Verbatim}[commandchars=\\\{\}]
hduser@ubuntu:\PYGZti{}\PYGZdl{} /usr/local/hadoop/bin/start\PYGZhy{}all.sh
\end{Verbatim}

This will startup a Namenode, Datanode, Jobtracker and a Tasktracker on the machine.

\begin{Verbatim}[commandchars=\\\{\}]
hduser@ubuntu:/usr/local/hadoop\PYGZdl{} jps
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{8.png}}
\end{figure}


\paragraph{Errors:}
\label{hadoop:errors}\begin{enumerate}
\item {} \begin{description}
\item[{If by chance your datanode is not starting, then you have to erase the contents of the folder /app/hadoop/tmp}] \leavevmode
The command that can be used

\begin{Verbatim}[commandchars=\\\{\}]
hduser@ubuntu:\PYGZti{}:\PYGZdl{} sudo rm –Rf /app/hadoop/tmp/*
\end{Verbatim}

\end{description}

\item {} \begin{description}
\item[{You can also check with netstat if Hadoop is listening on the configured ports.}] \leavevmode
The command that can be used

\begin{Verbatim}[commandchars=\\\{\}]
hduser@ubuntu:\PYGZti{}\PYGZdl{} sudo netstat \PYGZhy{}plten \textbar{} grep java
\end{Verbatim}

\end{description}

\item {} 
Errors if any, examine the log files in the /logs/ directory.

\end{enumerate}


\subsubsection{Stopping your single-node cluster}
\label{hadoop:stopping-your-single-node-cluster}
Run the command to stop all the daemons running on your machine.

\begin{Verbatim}[commandchars=\\\{\}]
hduser@ubuntu:\PYGZti{}\PYGZdl{} /usr/local/hadoop/bin/stop\PYGZhy{}all.sh
\end{Verbatim}


\paragraph{ERROR POINTS:}
\label{hadoop:error-points}
If datanode is not starting, then clear the tmp folder before formatting the namenode using the following command

\begin{Verbatim}[commandchars=\\\{\}]
hduser@ubuntu:\PYGZti{}\PYGZdl{} rm \PYGZhy{}Rf /app/hadoop/tmp/*
\end{Verbatim}


\paragraph{Note:}
\label{hadoop:id4}\begin{itemize}
\item {} 
The masters and slaves file should contain localhost.

\item {} 
In /etc/hosts, the ip of the system should be given with the alias as localhost.

\item {} 
Set the java home path in hadoop-env.sh as well bashrc.

\end{itemize}


\section{MULTI-NODE INSTALLATION}
\label{hadoop:multi-node-installation}

\subsection{Running Hadoop on Ubuntu Linux (Multi-Node Cluster)}
\label{hadoop:running-hadoop-on-ubuntu-linux-multi-node-cluster}

\subsubsection{From single-node clusters to a multi-node cluster}
\label{hadoop:from-single-node-clusters-to-a-multi-node-cluster}
We will build a multi-node cluster merge two or more single-node clusters into one multi-node cluster in which one Ubuntu box will become the designated master but also act as a slave , and the other box will become only a slave.
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{9.png}}
\end{figure}


\subsection{Prerequisites}
\label{hadoop:id5}
Configuring single-node clusters first,here we have used two single node clusters.
Shutdown each single-node cluster with the following command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  bin/stop\PYGZhy{}all.sh
\end{Verbatim}


\subsection{Networking}
\label{hadoop:networking}\begin{itemize}
\item {} 
The easiest is to put both machines in the same network with regard to hardware and   software configuration.

\item {} 
Update /etc/hosts on both machines .Put the alias to the ip addresses of all the machines. Here we are creating a cluster of 2 machines , one is master and other is slave 1

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZdl{}  cd /etc/hosts
\end{Verbatim}

\item {} 
Add the following lines for two node cluster

\begin{Verbatim}[commandchars=\\\{\}]
10.105.15.78    master  (IP address of the master node)
10.105.15.43    slave1   (IP address of the slave node)
\end{Verbatim}

\end{itemize}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{10.png}}
\end{figure}


\subsection{SSH access}
\label{hadoop:ssh-access}
The hduser user on the master (aka \href{mailto:hduser@master}{hduser@master}) must be able to connect:
\begin{enumerate}
\item {} 
to its own user account on the master - i.e. ssh master in this context.

\item {} 
to the hduser user account on the slave (i.e. \href{mailto:hduser@slave1}{hduser@slave1}) via a password-less SSH         login.

\end{enumerate}
\begin{itemize}
\item {} 
Add the \href{mailto:hduser@master}{hduser@master} public SSH key using the following command

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  ssh\PYGZhy{}copy\PYGZhy{}id \PYGZhy{}i \PYGZdl{}HOME/.ssh/id\PYGZus{}rsa.pub hduser@slave1
\end{Verbatim}

\end{itemize}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{11.png}}
\end{figure}
\begin{itemize}
\item {} 
Connect with user hduser from the master to the user account hduser on the slave.

\end{itemize}
\begin{enumerate}
\item {} 
From master to master

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  ssh master
\end{Verbatim}

\end{enumerate}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{12.png}}
\end{figure}
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
From master to slave

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  ssh slave1
\end{Verbatim}

\end{enumerate}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{13.png}}
\end{figure}


\subsection{Hadoop}
\label{hadoop:hadoop}

\subsubsection{Cluster Overview}
\label{hadoop:cluster-overview}
This will describe how to configure one Ubuntu box as a master node and the other Ubuntu box as a slave node.


\subsubsection{Configuration}
\label{hadoop:id6}

\paragraph{conf/masters}
\label{hadoop:conf-masters}
The machine on which bin/start-dfs.sh is running will become the primary NameNode.
This file should be updated on all the nodes. Open the masters file in the conf directory

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master/slave :\PYGZti{}\PYGZdl{} /usr/local/hadoop/conf
hduser@master/slave :\PYGZti{}\PYGZdl{} sudo gedit masters
\end{Verbatim}

Add the following line

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{Master}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{14.png}}
\end{figure}


\paragraph{conf/slaves}
\label{hadoop:conf-slaves}
This file should be updated on all the nodes as master is also a slave.
Open the slaves file in the conf directory

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master/slave:\PYGZti{}/usr/local/hadoop/conf\PYGZdl{} sudo gedit slaves
\end{Verbatim}

Add the following lines

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{Master}
\PYG{n}{Slave1}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{15.png}}
\end{figure}


\paragraph{conf/{\color{red}\bfseries{}*}-site.xml (all machines)}
\label{hadoop:conf-site-xml-all-machines}
Open this file in the conf directory

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}/usr/local/hadoop/conf\PYGZdl{} sudo gedit core\PYGZhy{}site.xml
\end{Verbatim}

Change the fs.default.name parameter (in conf/core-site.xml), which specifies the NameNode (the HDFS master) host and port.

conf/core-site.xml (ALL machines .ie. Master as well as slave)

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}fs.default.name\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}hdfs://master:54310\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}The name of the default file system.  A URI whose
    scheme and authority determine the FileSystem implementation.  The
    uri\PYGZsq{}s scheme determines the config property (fs.SCHEME.impl) naming
    the FileSystem implementation class.  The uri\PYGZsq{}s authority is used to
    determine the host, port, etc. for a filesystem.\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{16.png}}
\end{figure}


\paragraph{conf/mapred-site.xml}
\label{hadoop:conf-mapred-site-xml}
Open this file in the conf directory

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  /usr/local/hadoop/conf
hduser@master:\PYGZti{}\PYGZdl{}  sudo gedit mapred\PYGZhy{}site.xml
\end{Verbatim}

Change the mapred.job.tracker parameter (in conf/mapred-site.xml), which specifies the JobTracker (MapReduce master) host and port.

conf/mapred-site.xml (ALL machines)

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}mapred.job.tracker\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}master:54311\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}The host and port that the MapReduce job tracker runs
    at.  If \PYGZdq{}local\PYGZdq{}, then jobs are run in\PYGZhy{}process as a single map
    and reduce task.
    \PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{17.png}}
\end{figure}


\paragraph{conf/hdfs-site.xml}
\label{hadoop:conf-hdfs-site-xml}
Open this file in the conf directory

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  /usr/local/hadoop/conf
hduser@master:\PYGZti{}\PYGZdl{}  sudo gedit hdfs\PYGZhy{}site.xml
\end{Verbatim}

Change the dfs.replication parameter (in conf/hdfs-site.xml) which specifies the default block replication.
We have two nodes available, so we set dfs.replication to 2.


\paragraph{conf/hdfs-site.xml (ALL machines)}
\label{hadoop:conf-hdfs-site-xml-all-machines}
Changes to be made

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
\PYGZlt{}name\PYGZgt{}dfs.replication\PYGZlt{}/name\PYGZgt{}
\PYGZlt{}value\PYGZgt{}2\PYGZlt{}/value\PYGZgt{}
\PYGZlt{}description\PYGZgt{}Default block replication.
    The actual number of replications can be specified when the file is created.
    The default is used if replication is not specified in create time.
    \PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{18.png}}
\end{figure}


\subsubsection{Formatting the HDFS filesystem via the NameNode}
\label{hadoop:id9}
Format the cluster’s HDFS file system

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}/usr/local/hadoop\PYGZdl{} bin/hadoop namenode \PYGZhy{}format
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{19.png}}
\end{figure}


\subsubsection{Starting the multi-node cluster}
\label{hadoop:starting-the-multi-node-cluster}
Starting the cluster is performed in two steps.
\begin{enumerate}
\item {} 
We begin with starting the HDFS daemons: the NameNode daemon is started on master, and DataNode daemons are started on all slaves (here: master and slave).

\item {} 
Then we start the MapReduce daemons: the JobTracker is started on master, and TaskTracker daemons are started on all slaves (here: master and slave).

\end{enumerate}

Cluster is started by running the commnd on master

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}   /usr/local/hadoop
hduser@master:\PYGZti{}\PYGZdl{}   bin/start\PYGZhy{}all.sh
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{20.png}}
\end{figure}

By this command:
\begin{itemize}
\item {} 
The NameNode daemon is started on master, and DataNode daemons are started on all slaves (here: master and slave).

\item {} 
The JobTracker is started on master, and TaskTracker daemons are started on all slaves (here: master and slave)

\end{itemize}

To check the daemons running , run the following commands

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  jps
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{21.png}}
\end{figure}

On slave, datanode and jobtracker should run.

\begin{Verbatim}[commandchars=\\\{\}]
hduser@slave:\PYGZti{}/usr/local/hadoop\PYGZdl{} jps
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{22.png}}
\end{figure}


\subsubsection{Stopping the multi-node cluster}
\label{hadoop:stopping-the-multi-node-cluster}
To stop the multinode cluster , run the following command on master pc

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{} cd /usr/local/hadoop
hduser@master:\PYGZti{}/usr/local/hadoop\PYGZdl{} bin/stop\PYGZhy{}all.sh
\end{Verbatim}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{23.png}}
\end{figure}


\paragraph{ERROR POINTS:}
\label{hadoop:id10}\begin{enumerate}
\item {} \begin{description}
\item[{Number of slaves = Number of replications in hdfs-site.xml}] \leavevmode
also number of slaves = all slaves + master(if master is also considered to be a slave)

\end{description}

\item {} 
When you start the cluster, clear the tmp directory on all the nodes (master+slaves) using the following command

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}   rm \PYGZhy{}Rf /app/hadoop/tmp/*
\end{Verbatim}

\item {} 
Configuration of /etc/hosts , masters  and slaves files on both the masters and the slaves nodes should be the same.

\item {} 
If namenode is not getting started run the following commands:
\begin{itemize}
\item {} 
To give all permissions  of hadoop folder to hduser

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  sudo chmod \PYGZhy{}R 777 /app/hadoop
\end{Verbatim}

\item {} 
This command deletes the junk files which gets stored in tmp folder of hadoop

\begin{Verbatim}[commandchars=\\\{\}]
hduser@master:\PYGZti{}\PYGZdl{}  sudo rm \PYGZhy{}Rf /app/hadoop/tmp/*
\end{Verbatim}

\end{itemize}

\end{enumerate}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{24.png}}
\end{figure}


\chapter{HIVE INSTALLATION}
\label{hive:hive-installation}\label{hive::doc}
This section refers to the installation settings of Hive on a standalone system
as well as on a system existing as a node in a cluster.


\section{INTRODUCTION}
\label{hive:introduction}\begin{quote}

Apache Hive is a data warehouse infrastructure built on top of Hadoop for providing data summarization, query, and analysis. Apache Hive supports analysis of large datasets stored in Hadoop's HDFS and compatible file systems such as Amazon S3 filesystem. It provides an SQL-like language called HiveQL(Hive Query Language) while maintaining full support for map/reduce.
\end{quote}


\section{Hive Installation}
\label{hive:id1}

\subsection{Installing HIVE:}
\label{hive:installing-hive}\begin{itemize}
\item {} 
Browse to the link: \href{http://apache.claz.org/hive/stable/}{http://apache.claz.org/hive/stable/}

\item {} 
Click the apache-hive-0.13.0-bin.tar.gz

\item {} 
Save and Extract it
\begin{quote}

Commands

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  cd  /usr/lib/
user@ubuntu:\PYGZti{}\PYGZdl{}  sudo mkdir hive
user@ubuntu:\PYGZti{}\PYGZdl{}  cd Downloads
user@ubuntu:\PYGZti{}\PYGZdl{}  sudo mv apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin /usr/lib/hive
\end{Verbatim}
\end{quote}

\end{itemize}


\subsection{Setting Hive environment variable:}
\label{hive:setting-hive-environment-variable}
Commands

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  cd
user@ubuntu:\PYGZti{}\PYGZdl{}  sudo gedit  \PYGZti{}/.bashrc
\end{Verbatim}

Copy and paste the following lines at end of the file

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} Set HIVE\PYGZus{}HOME
export HIVE\PYGZus{}HOME=\PYGZdq{}/usr/lib/hive/apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin\PYGZdq{}
PATH=\PYGZdl{}PATH:\PYGZdl{}HIVE\PYGZus{}HOME/bin
export PATH
\end{Verbatim}


\subsection{Setting HADOOP\_PATH in HIVE config.sh}
\label{hive:setting-hadoop-path-in-hive-config-sh}
Commands

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} cd  /usr/lib/hive/apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin/bin
user@ubuntu:\PYGZti{}\PYGZdl{} sudo gedit hive\PYGZhy{}config.sh
\end{Verbatim}

Go to the line where the following statements are written

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} Allow alternate conf dir location.
HIVE\PYGZus{}CONF\PYGZus{}DIR=\PYGZdq{}\PYGZdl{}\PYGZob{}HIVE\PYGZus{}CONF\PYGZus{}DIR:\PYGZhy{}\PYGZdl{}HIVE\PYGZus{}HOME/conf\PYGZdq{}
export HIVE\PYGZus{}CONF\PYGZus{}DIR=\PYGZdl{}HIVE\PYGZus{}CONF\PYGZus{}DIR
export HIVE\PYGZus{}AUX\PYGZus{}JARS\PYGZus{}PATH=\PYGZdl{}HIVE\PYGZus{}AUX\PYGZus{}JARS\PYGZus{}PATH
\end{Verbatim}

Below this write the following

\begin{Verbatim}[commandchars=\\\{\}]
export HADOOP\PYGZus{}HOME=/usr/local/hadoop    (write the path where hadoop file is there)
\end{Verbatim}


\subsection{Create Hive directories within HDFS}
\label{hive:create-hive-directories-within-hdfs}
Command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}   hadoop fs \PYGZhy{}mkdir /usr/hive/warehouse
\end{Verbatim}


\subsection{Setting READ/WRITE permission for table}
\label{hive:setting-read-write-permission-for-table}
Command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  hadoop fs \PYGZhy{}chmod g+w /usr/hive/warehouse
\end{Verbatim}


\subsection{HIVE launch}
\label{hive:hive-launch}
Command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  hive
\end{Verbatim}

Hive shell will prompt:


\subsubsection{OUTPUT}
\label{hive:output}
Shell will look like

\begin{Verbatim}[commandchars=\\\{\}]
Logging initialized using configuration in jar:file:/usr/lib/hive/apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin/lib/hive\PYGZhy{} common\PYGZhy{}0.13.0.jar!/hive\PYGZhy{}log4j.properties
hive\PYGZgt{}
\end{Verbatim}


\subsection{Creating a database}
\label{hive:creating-a-database}
Command

\begin{Verbatim}[commandchars=\\\{\}]
hive\PYGZgt{} create database mydb;
\end{Verbatim}

OUTPUT

\begin{Verbatim}[commandchars=\\\{\}]
OK
Time taken: 0.369 seconds
hive\PYGZgt{}
\end{Verbatim}


\subsection{Configuring hive-site.xml:}
\label{hive:configuring-hive-site-xml}
Open with text-editor and change the following property

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}hive.metastore.local\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}TRUE\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}controls whether to connect to remove metastore server or open a new metastore server in Hive Client JVM\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}

\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}javax.jdo.option.ConnectionURL\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}jdbc:mysql://usr/lib/hive/apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin/metastore\PYGZus{}db? createDatabaseIfNotExist=true\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}JDBC connect string for a JDBC metastore\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}

\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}javax.jdo.option.ConnectionDriverName\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}com.mysql.jdbc.Driver\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}Driver class name for a JDBC metastore\PYGZlt{}/description\PYGZgt{}
\PYGZlt{}/property\PYGZgt{}

\PYGZlt{}property\PYGZgt{}
    \PYGZlt{}name\PYGZgt{}hive.metastore.warehouse.dir\PYGZlt{}/name\PYGZgt{}
    \PYGZlt{}value\PYGZgt{}/usr/hive/warehouse\PYGZlt{}/value\PYGZgt{}
    \PYGZlt{}description\PYGZgt{}location of default database for the warehouse\PYGZlt{}/description\PYGZgt{}
 \PYGZlt{}/property\PYGZgt{}
\end{Verbatim}


\subsection{Writing a Script}
\label{hive:writing-a-script}
Open a new terminal (CTRL+ALT+T)

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}      sudo gedit sample.sql

create database sample;
use sample;
create table product(product int, productname string, price float)[row format delimited fields terminated by \PYGZsq{},\PYGZsq{};]
describe product;
\end{Verbatim}

load data local inpath `/home/hduser/input\_to\_product.txt' into table product

\begin{Verbatim}[commandchars=\\\{\}]
select * from product;
\end{Verbatim}

SAVE and CLOSE

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo gedit input\PYGZus{}to\PYGZus{}product.txt
user@ubuntu:\PYGZti{}\PYGZdl{} cd /usr/lib/hive/apache\PYGZhy{}hive\PYGZhy{}0.13.0\PYGZhy{}bin/ \PYGZdl{} bin/hive \PYGZhy{}f /home/hduser/sample.sql
\end{Verbatim}


\chapter{SQOOP INSTALLATION}
\label{sqoop::doc}\label{sqoop:sqoop-installation}
This section refers to the installation settings of Sqoop.


\section{INTRODUCTION}
\label{sqoop:introduction}\begin{itemize}
\item {} 
Sqoop is a tool designed to transfer data between Hadoop and relational databases.

\item {} 
You can use Sqoop to import data from a relational database management system (RDBMS) such as MySQL or Oracle into the Hadoop Distributed File System (HDFS), transform the data in Hadoop MapReduce, and then export the data back into an RDBMS. Sqoop automates most of this process, relying on the database to describe the schema for the data to be imported. Sqoop uses MapReduce to import and export the data, which provides parallel operation as well as fault tolerance. This document describes how to get started using Sqoop to move data between databases and Hadoop and provides reference information for the operation of the Sqoop command-line tool suite.

\end{itemize}
\begin{figure}[htbp]
\centering

\scalebox{0.500000}{\includegraphics{25.png}}
\end{figure}


\section{Stable release and Download}
\label{sqoop:stable-release-and-download}
Sqoop is an open source software product of the Apache Software Foundation.
Sqoop source code is held in the Apache Git repository.


\section{Prerequisites}
\label{sqoop:prerequisites}
Before we can use Sqoop, a release of Hadoop must be installed and con?gured. Sqoop is currently supporting 4 major Hadoop releases - 0.20, 0.23, 1.0 and 2.0. We have installed Hadoop 2.2.0 and it is compatible with sqoop 1.4.4.We are using a Linux environment Ubuntu 12.04 to install and run sqoop. The basic familiarity with the purpose and operation of Hadoop is required to use this product.


\section{Installation}
\label{sqoop:installation}
To install the sqoop 1.4.4 we followed the given sequence of steps :
\begin{enumerate}
\item {} 
Download the sqoop-1.4.4.bin\_hadoop-1.0.0.tar.gz  file from
www.apache.org/dyn/closer.cgl/sqoop/1.4.4

\item {} 
Unzip the tar ?le: sudo tar -zxvf sqoop-1.4.4.bin hadoop1.0.0.tar.gz

\item {} 
Move sqoop-1.4.4.bin hadoop1.0.0 to sqoop using command

\href{mailto:user@ubuntu}{user@ubuntu}:\textasciitilde{}\$  sudo mv sqoop  1.4.4.bin hadoop1.0.0 /usr/local/sqoop

\item {} 
Create a directory sqoop in usr/lib using command

\href{mailto:user@ubuntu}{user@ubuntu}:\textasciitilde{}\$ sudo mkdir /usr/lib/sqoop

\item {} 
Go to the zipped folder sqoop-1.4.4.bin\_hadoop-1.0.0 and run the command

\href{mailto:user@ubuntu}{user@ubuntu}:\textasciitilde{}sudo mv ./* /usr/lib/sqoop

\item {} 
Go to root directory using cd command

\href{mailto:user@ubuntu}{user@ubuntu}:\textasciitilde{}\$  cd

\item {} 
Open bashrc file using

\href{mailto:user@ubuntu}{user@ubuntu}:\textasciitilde{}\$  sudo gedit \textasciitilde{}/.bashrc

\item {} 
Add the following lines

\begin{Verbatim}[commandchars=\\\{\}]
export SQOOP\PYGZus{}HOME=¡usr/lib/sqoop
export PATH=\PYGZdl{}PATH:\PYGZdl{}SQOOP\PYGZus{}HOME/bin
\end{Verbatim}

\item {} 
To check if the sqoop has been installed  successfully type the command

\begin{Verbatim}[commandchars=\\\{\}]
sqoop version
\end{Verbatim}

\end{enumerate}


\chapter{IMPORTING DATA FROM HADOOP TO MYSQL}
\label{importdata:importing-data-from-hadoop-to-mysql}\label{importdata::doc}

\section{Steps to install mysql}
\label{importdata:steps-to-install-mysql}\begin{itemize}
\item {} 
Run the command :sudo apt-get install mysql-server and give appropriate username and password.

\end{itemize}


\section{Using sqoop to perform import to hadoop from sql}
\label{importdata:using-sqoop-to-perform-import-to-hadoop-from-sql}\begin{enumerate}
\item {} 
Download mysql-connector-java-5.1.28-bin.jar and move to /usr/lib/sqoop/lib using command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sudo cp mysql\PYGZhy{}connnectpr\PYGZhy{}java\PYGZhy{}5.1.28\PYGZhy{}bin.jar /usr/lib/sqoop/lib/
\end{Verbatim}

\item {} 
Login to mysql using command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}   mysql \PYGZhy{}u root \PYGZhy{}p
\end{Verbatim}

\item {} 
Login to secure shell using command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  ssh localhost
\end{Verbatim}

\item {} 
Start hadoop using the command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}  bin/hadoop start\PYGZhy{}all.sh
\end{Verbatim}

\item {} 
Run the command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{} sqoop import \PYGZhy{}connect jdbc:mysql://localhost:3306/sqoop \PYGZhy{}username root \PYGZhy{}pasword abc \PYGZhy{}table employees \PYGZhy{}m
\end{Verbatim}

\end{enumerate}

This command imports the employees table from the sqoop directory of myql to hdfs.


\section{Error points}
\label{importdata:error-points}\begin{enumerate}
\item {} 
Do check if the hadoop is in safe mode using command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}hadoop dfsadmin \PYGZhy{}safemode get
\end{Verbatim}

\end{enumerate}

If you are getting safemode is on, run the command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}hadoop dfsadmin \PYGZhy{}safemode leave
\end{Verbatim}

and again run the command

\begin{Verbatim}[commandchars=\\\{\}]
user@ubuntu:\PYGZti{}\PYGZdl{}hadoop dfsadmin \PYGZhy{}safemode get
\end{Verbatim}

and confirm that you are getting safemode is off.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Do make sure that haoop is running before performing the import action.

\end{enumerate}


\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\emph{genindex}

\item {} 
\emph{modindex}

\item {} 
\emph{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}
